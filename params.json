{"name":"Practmachlearn","tagline":"Coursera Practical Machine Learning","body":"#Synopsis  \r\n\r\nUsing monitoring devices such as Jawbone Up, Nike FuelBand, and Fitbit allows the user to collect a large amount of data about personal activity. These devices are used to regularly quantify how much of a particular activity done; however, they rarely quantify *how well* the activity was completed. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways and predict the manner in which they did the exercise using the \"classe\" variable in the training dataset. \r\n\r\n```{r setup, include = FALSE}\r\nknitr::opts_chunk$set(cache=TRUE)\r\n## If you run into problems with cached output you can always clear the knitr cache by removing the folder named with \r\n## a _cache suffix within your document’s directory\r\nlibrary(\"knitr\")\r\n```\r\n\r\nRead in the data: \r\n\r\n```{r, echo=FALSE}\r\ntraining <- read.csv(\"pml-training.csv\", stringsAsFactors = TRUE)\r\ntesting <- read.csv(\"pml-testing.csv\", stringsAsFactors = TRUE)\r\nstr(training)\r\n\r\n```\r\n\r\n#Building Models\r\nWhen building a model, the goal is to accurately predict the \"classe\" output in the data set.  Class \"A\" corresponds to executing the exercise properly while the other classes, \"B\" through \"E\", correspond to common mistakes. We'll first look at the \"classe\" data to see what we're confronting.  We'll also go ahead and split the training data into a 90/10 grouping so that we can use 10% of the training set for cross-validation.  \r\n\r\n\r\n```{r, echo=FALSE}\r\nsummary(training$classe)\r\nstr(training$classe)\r\nlibrary(\"caret\")\r\nlibrary(\"lattice\")\r\nlibrary(\"ggplot2\")\r\ninTrain <- createDataPartition(y=training$classe, p=0.9, list=FALSE)\r\ntrainingSub <- training[inTrain,]\r\ntestingSub <- training[-inTrain,]\r\n\r\n```\r\n\r\nNext we'll use the gbm package to run our data and create our model.  I did try running another modelling tecnique, but it took quite a long time on my work laptop (Intel i7 processor with 16 GB of RAM, though I doubt R accessed anything in that neighborhood).\r\n\r\n\r\n```{r}\r\nset.seed(12358)\r\nsystem.time(modFit <- train(classe ~ user_name + pitch_arm + yaw_arm + roll_arm + roll_belt + pitch_belt + yaw_belt +\r\n                                gyros_belt_x + gyros_belt_y + gyros_belt_z + accel_belt_x + accel_belt_y + accel_belt_z +\r\n                                magnet_belt_x + magnet_belt_y + magnet_belt_z + gyros_arm_x + gyros_arm_y + gyros_arm_z +\r\n                                accel_arm_x + accel_arm_y + accel_arm_z + magnet_arm_x + magnet_arm_y + magnet_arm_z +\r\n                                roll_dumbbell + pitch_dumbbell + yaw_dumbbell, method=\"gbm\", data=trainingSub, verbose=FALSE))\r\nprint(modFit)\r\n```\r\n\r\n\r\n\r\n```{r}\r\npredictTr <- predict(modFit,trainingSub)\r\ntable(predictTr, trainingSub$classe)\r\nsummary(modFit,n.trees=150)\r\n\r\n```\r\n\r\n\r\nThe model correctly classifies over 93% of the observations in the training sub-sample using 150 trees. The *roll_belt* and *yaw_belt* features appear to be the most influential.  \r\n\r\nWe can look at a plot of these top two features colored by outcome.\r\n\r\n\r\nThey are not the best predictors on their own, but the plot shows groupings. This confirms that a boosting algorithm was a good choice since there is a large set of relatively weak predictors. This second plot further demonstrates the improved performance gained by using boosting iterations.\r\n\r\n\r\n```{r}\r\nqplot(roll_belt, yaw_belt,colour=classe,data=trainingSub)\r\nggplot(modFit)\r\n```\r\n\r\nNow, I have to check the performance on the 10 percent subsample to get an estimate of the algorithm’s out-of-sample performance.\r\n\r\n```{r}\r\npredictTe <- predict(modFit,testingSub)\r\ntable(predictTe, testingSub$classe)\r\n```\r\n\r\nThe algorithm performs very close on the 10 percent, performing slightly worse (by about 0.2%) on this subset than it did on the full training set, correctly classifying 93.4% of the observations.  \r\n\r\n\r\n```{r}\r\n#pml.testing <- read.csv(\"~/R/Coursera /Machine Learning/Project/pml-testing.csv\")\r\n#answers <- as.character(predict(modFit, pml.testing))\r\n#pml_write_files = function(x){\r\n#  n = length(x)\r\n#  for(i in 1:n){\r\n#    filename = paste0(\"problem_id_\",i,\".txt\")\r\n#    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n#  }\r\n#}\r\n#pml_write_files(answers)\r\n```\r\n\r\n\r\n\r\n#References  \r\n[see Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6]. \r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}